{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 and 2: Simple Neural Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40460\\393293057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUClasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import UClasses\n",
    "import copy\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UClasses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40460\\1954334085.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UClasses' is not defined"
     ]
    }
   ],
   "source": [
    "ds = UClasses(n=1000, binary=False)\n",
    "ds.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Operation` class: Activation/Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "     Operation class\n",
    "     \n",
    "     This is the abstract base class that other operations should be based on.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Identity(Operation):\n",
    "    '''\n",
    "     act = Identity()\n",
    "     \n",
    "     Creates an Operation object that represents the identity mapping.\n",
    "     \n",
    "     Usage:\n",
    "      act = Identity()\n",
    "      act(np.array([[1.2, 5.]]))\n",
    "     produces the numpy array\n",
    "      [[1.2, 5.]]\n",
    "    '''\n",
    "    def __call__(self, z):\n",
    "        '''\n",
    "         y = act(z)\n",
    "         \n",
    "         Evaluates the identity function, element-by-element, on z.\n",
    "         \n",
    "         Input:\n",
    "          z  is a numpy array\n",
    "         Output:\n",
    "          y  is a numpy array the same size as z\n",
    "        '''\n",
    "        self.dims = z.shape\n",
    "        y = copy.deepcopy(z)\n",
    "        return y\n",
    "    \n",
    "    def derivative(self, s=None):\n",
    "        '''\n",
    "         act.derivative(s=None)\n",
    "         \n",
    "         Computes the derivative of the identity mapping\n",
    "         element-by-element.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "           \n",
    "         Output:\n",
    "           dactdz  array the same size as z when __call__ was called,\n",
    "                   and is s times the derivative\n",
    "           \n",
    "         Usage:\n",
    "           dactdz = act.derivative()\n",
    "           dactdz = act.derivative(s)\n",
    "        '''\n",
    "        # Compute the derivatives\n",
    "        if s is None:\n",
    "            return np.ones(self.dims)\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "\n",
    "class Softmax(Operation):\n",
    "    '''\n",
    "     act = Softmax()\n",
    "\n",
    "     Creates an Operation object that represents the softmax\n",
    "     function. The softmax is applied to the rows of the input.\n",
    "\n",
    "     Usage:\n",
    "      act = Softmax()\n",
    "      act(np.array([[0., 0.5]]))\n",
    "     produces the numpy array\n",
    "      [0.37754067 0.62245933]\n",
    "    '''\n",
    "    def __call__(self, z):\n",
    "        v = np.exp(z)\n",
    "        # Compute denominator (sum along rows)\n",
    "        denom = np.sum(v, axis=1)\n",
    "        # Softmax formula, duplicating the denom across rows\n",
    "        self.y = v/np.tile(denom[:,np.newaxis], [1,np.shape(v)[1]])\n",
    "        # Store self.y so you can use it in derivative\n",
    "        return self.y\n",
    "\n",
    "    def derivative(self, s):\n",
    "        '''\n",
    "         dydz = act.derivative(s)\n",
    "\n",
    "         Computes the derivative of the softmax function.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "\n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "                   NOTE: s is a *mandatory* argument (not optional)\n",
    "                   NOTE: s should have only a single non-zero element\n",
    "\n",
    "         Output:\n",
    "           dydz    array the same size as z when __call__ was called,\n",
    "                   and is s Hadamard-times the derivative\n",
    "\n",
    "         Usage:\n",
    "           dydz = act.derivative(s)\n",
    "        '''\n",
    "        idx = np.nonzero(s)[1]  # Find one-hot categories\n",
    "\n",
    "        # Create empty copies to populate\n",
    "        s_gamma = np.zeros_like(s)\n",
    "        y_gamma = np.zeros_like(self.y)\n",
    "        kronecker = np.zeros_like(s)\n",
    "\n",
    "        # Compute dy_k/dz_j \n",
    "        for j,gamma in enumerate(idx):\n",
    "            s_gamma[j,:] = s[j,gamma]\n",
    "            y_gamma[j,:] = self.y[j,gamma]\n",
    "            kronecker[j,gamma] = 1.\n",
    "        dydz = s_gamma*y_gamma*(kronecker-self.y)\n",
    "        return dydz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MSE(Operation):\n",
    "    '''\n",
    "     E = MSE()\n",
    "     \n",
    "     Creates an object that implements the mean squared error loss.\n",
    "     \n",
    "     Usage:\n",
    "      E = MSE()\n",
    "      loss = E(y, t)\n",
    "      \n",
    "     Example:\n",
    "      y = np.array([[0.5, 0.1],[-0.4, 0.9], [-0.1, 0.4]])\n",
    "      t = np.array([[0.6, 0.1],[-0.4, 0.7], [-0.1, 0.6]])\n",
    "      loss = E(y, t)\n",
    "     produces the value\n",
    "      0.015  since it equals\n",
    "      (0.1^2 + 0.2^2 + 0.2^2)/2 / 3\n",
    "    '''\n",
    "    def __call__(self, y, t):\n",
    "        '''\n",
    "         E.__call__(y, t)  or   E(y, t)\n",
    "         \n",
    "         Computes the mean (average) squared error between the outputs\n",
    "         y and the targets t.\n",
    "         \n",
    "         Inputs:\n",
    "           y  array with one sample per row\n",
    "           t  array the same size as y\n",
    "           \n",
    "         Output:\n",
    "           loss  MSE loss (scalar)\n",
    "        '''\n",
    "        # MSE formula\n",
    "        self.n_samples = np.shape(t)[0]\n",
    "        L = np.sum((y-t)**2)/2./self.n_samples\n",
    "        self.dL = (y-t) / self.n_samples\n",
    "        return L\n",
    "\n",
    "    def derivative(self):\n",
    "        '''\n",
    "         E.derivative()\n",
    "         \n",
    "         Computes and the derivative of the MSE with respect to y.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Output:\n",
    "           dEdy  array the same size as y when __call__ was called\n",
    "        '''\n",
    "        # Compute the gradient of MSE w.r.t. network output\n",
    "        return self.dL\n",
    "\n",
    "        \n",
    "class CategoricalCE(Operation):\n",
    "    '''\n",
    "     E = CrossEntropy()\n",
    "\n",
    "     Creates an object that implements the average cross-entropy loss.\n",
    "\n",
    "     Usage:\n",
    "      E = CrossEntropy()\n",
    "      loss = E(y, t)\n",
    "    '''\n",
    "    def __call__(self, y, t):\n",
    "        '''\n",
    "         E.__call__(y, t)  or   E(y, t)\n",
    "\n",
    "         Computes the average categorial cross-entropy between the outputs\n",
    "         y and the targets t.\n",
    "\n",
    "         Inputs:\n",
    "           y  array with one sample per row\n",
    "           t  array the same size as y\n",
    "\n",
    "         Output:\n",
    "           loss  average categorical CE loss (scalar)\n",
    "        '''\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        return -np.sum(t * np.log(y)) / len(t)\n",
    "        \n",
    "    def derivative(self):\n",
    "        '''\n",
    "         E.derivative()\n",
    "\n",
    "         Computes and the derivative of categorical CE with respect to y.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "\n",
    "         Output:\n",
    "           dEdy  array the same size as y when __call__ was called\n",
    "        '''\n",
    "        return -self.t/self.y / len(self.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a) Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Operation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_40460\\182091783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     '''\n\u001b[0;32m      3\u001b[0m      \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[0mCreates\u001b[0m \u001b[0man\u001b[0m \u001b[0mOperation\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Operation' is not defined"
     ]
    }
   ],
   "source": [
    "def logistic_function(x):\n",
    "  return 1/(1+np.exp(-1 * x))\n",
    "\n",
    "class Logistic(Operation):\n",
    "    '''\n",
    "     act = Logistic()\n",
    "     \n",
    "     Creates an Operation object that represents the logistic\n",
    "     function.\n",
    "     \n",
    "     Usage:\n",
    "      act = Logistic()\n",
    "      act(np.array([0., 0.5]))\n",
    "     produces the numpy array\n",
    "      [0.5 , 0.62245933]\n",
    "    '''\n",
    "    def __call__(self, z):\n",
    "        '''\n",
    "         y = act(z)\n",
    "         \n",
    "         Evaluates the logistic function, element-by-element, on z.\n",
    "         \n",
    "         Input:\n",
    "          z  is a numpy array\n",
    "          \n",
    "         Output:\n",
    "          y  is a numpy array the same size as z\n",
    "        '''\n",
    "        #===== YOUR CODE HERE =====\n",
    "        self.dims = z.shape\n",
    "        self.z = z\n",
    "        return 1 / (1 + np.exp(-1 * z))  # replace this line\n",
    "    \n",
    "    def derivative(self, s=None):\n",
    "        '''\n",
    "         act.derivative(s=None)\n",
    "         \n",
    "         Computes the derivative of the logistic function\n",
    "         element-by-element.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "                   If s is None (or omitted), an array of 1s will be used.\n",
    "           \n",
    "         Output:\n",
    "           dydz    array the same size as z when __call__ was called,\n",
    "                   containing the derivative, multiplied by s\n",
    "\n",
    "         Usage:\n",
    "           dydz = act.derivative()\n",
    "           dydz = act.derivative(s)\n",
    "        '''\n",
    "        #===== YOUR CODE HERE =====\n",
    "        if s is None:\n",
    "            return np.ones(self.dims) \n",
    "        else:\n",
    "            return s * (np.exp(-1 * self.z) / (1 + np.exp(-1 * self.z))**2)\n",
    "\n",
    "act = Logistic()\n",
    "#print(act(np.array([0., 0.5])))\n",
    "#print(act.derivative())\n",
    "#print(act.derivative(np.array([-0.5, 0, 0.5, 1.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b) Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Operation):\n",
    "    '''\n",
    "     E = CrossEntropy()\n",
    "     \n",
    "     Creates an object that implements the average cross-entropy loss.\n",
    "     \n",
    "     Usage:\n",
    "      E = CrossEntropy()\n",
    "      loss = E(y, t)\n",
    "    '''\n",
    "    def __call__(self, y, t):\n",
    "        '''\n",
    "         E.__call__(y, t)  or   E(y, t)\n",
    "         \n",
    "         Computes the average cross-entropy between the outputs\n",
    "         y and the targets t.\n",
    "         \n",
    "         Inputs:\n",
    "           y  array with one sample per row\n",
    "           t  array the same size as y\n",
    "           \n",
    "         Output:\n",
    "           loss  average CE loss (scalar)\n",
    "        '''\n",
    "        #===== YOUR CODE HERE =====\n",
    "        L = 0.   # replace this line\n",
    "\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        return -np.sum(t * np.log(y) + (1 - t) * np.log(1 - y)) / len(t)\n",
    "\n",
    "    def derivative(self):\n",
    "        '''\n",
    "         E.derivative(s=1)\n",
    "         \n",
    "         Computes the derivative of cross-entropy with respect to y.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Output:\n",
    "           dEdy  array the same size as y when __call__ was called\n",
    "        '''\n",
    "        #===== YOUR CODE HERE =====\n",
    "        return -((self.t / self.y) + (1-self.t)/(1-self.y)) / len(self.t)   # replace this line\n",
    "\n",
    "E = CrossEntropy()\n",
    "y = np.array([0.21, 0.89, 0.11])\n",
    "t = np.array([0, 1, 0])\n",
    "loss = E(y, t)\n",
    "print(loss)\n",
    "print(E.derivative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# `Layer` Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "     Layer is an abstract base class for different\n",
    "     types of layers.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Population(Layer):\n",
    "    '''\n",
    "     lyr = Population(nodes, act=Identity())\n",
    "\n",
    "     Creates a Population layer object.\n",
    "\n",
    "     Inputs:\n",
    "       nodes  the number of nodes in the population\n",
    "       act    activation function (Operation object)\n",
    "       \n",
    "     Usage:\n",
    "       lyr = Population(3, act=Logistic())\n",
    "       h = lyr(z)\n",
    "       print(lyr())   # prints current value of lyr.h\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nodes, act=Identity()):\n",
    "        self.nodes = nodes\n",
    "        self.z = None\n",
    "        self.h = None\n",
    "        self.act = act\n",
    "        self.params = []\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is not None:\n",
    "            self.z = x\n",
    "            self.h = self.act(x)\n",
    "        return self.h\n",
    "\n",
    "\n",
    "class Connection(Layer):\n",
    "    '''\n",
    "     lyr = Connection(from_nodes=1, to_nodes=1)\n",
    "\n",
    "     Creates a layer of all-to-all connections.\n",
    "\n",
    "     Inputs:\n",
    "       from_nodes  number of nodes in source layer\n",
    "       to_nodes    number of nodes in receiving layer\n",
    "\n",
    "     Usage:\n",
    "       lyr = Connection(from_nodes=3, to_nodes=5)\n",
    "       z = lyr(h)\n",
    "       lyr.W    # matrix of connection weights\n",
    "       lyr.b    # vector of biases\n",
    "    '''\n",
    "\n",
    "    def __init__(self, from_nodes=1, to_nodes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = np.random.randn(from_nodes, to_nodes) / np.sqrt(from_nodes)\n",
    "        self.b = np.zeros(to_nodes)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is None:\n",
    "            print('Should not call Connection without arguments.')\n",
    "            return\n",
    "        P = len(x)\n",
    "        if P>1:\n",
    "            return x@self.W + np.outer(np.ones(P), self.b)\n",
    "        else:\n",
    "            return x@self.W + self.b\n",
    "\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    '''\n",
    "     lyr = DenseLayer(from_nodes=1, to_nodes=1, act=Logistic())\n",
    "\n",
    "     Creates a DenseLayer object, composed of 2 layer objects:\n",
    "       L1  a Connection layer of connection weights, and\n",
    "       L2  a Population layer, consisting of nodes that receives current\n",
    "           from the Connection layer, and apply the activation function\n",
    "\n",
    "     Inputs:\n",
    "       from_nodes  how many nodes are in the layer below\n",
    "       to_nodes    how many nodes are in the new Population layer\n",
    "       act         activation function (Operation object)\n",
    "       \n",
    "     Usage:\n",
    "       lyr = DenseLayer(from_nodes=3, to_nodes=5)\n",
    "       h2 = lyr(h1)\n",
    "       lyr.L1.W        # connection weights\n",
    "       lyr.L2()        # activities of layer\n",
    "       lyr.L2.act      # activation function of layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, from_nodes=1, to_nodes=1, act=Logistic()):\n",
    "        self.L1 = Connection(from_nodes=from_nodes, to_nodes=to_nodes)\n",
    "        self.L2 = Population(to_nodes, act=act)\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is None:\n",
    "            return self.L2.h\n",
    "        else:\n",
    "            # Calculate and return the operation of the two layers, L1 and L2\n",
    "            return self.L2(self.L1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Question 3: `Network` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''\n",
    "     net = Network()\n",
    "\n",
    "     Creates a Network object.\n",
    "     \n",
    "     Usage:\n",
    "       net = Network()\n",
    "       net.add_layer(L)\n",
    "       ... (add more layers)\n",
    "       y = net(x)\n",
    "       net.lyr[1]    # reference to Layer object\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lyr = []\n",
    "        self.loss = None\n",
    "\n",
    "    def add_layer(self, L):\n",
    "        '''\n",
    "         net.add_layer(L)\n",
    "         \n",
    "         Adds the layer object L to the network.\n",
    "         \n",
    "         Note: It is up to the user to make sure the Layer object\n",
    "               fits with adjacent layers.\n",
    "        '''\n",
    "        self.lyr.append(L)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "         y = net(x)\n",
    "         \n",
    "         Feedforward pass of the network.\n",
    "         \n",
    "         Input:\n",
    "           x  batch of inputs, one input per row\n",
    "           \n",
    "         Output:\n",
    "           y  corresponding outputs, one per row\n",
    "        '''\n",
    "        for l in self.lyr:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def backprop(self, t, lrate=1.0):\n",
    "        '''\n",
    "         net.backprop(t, lrate=1.)\n",
    "         \n",
    "         Using the error between the state of the output layer and\n",
    "         the targets, this method does a backprop pass, and updates\n",
    "         the connection weights and biases.\n",
    "         \n",
    "         NOTE: This method assumes that the network is in the\n",
    "               correct state, following a feedforward pass.\n",
    "         \n",
    "         Inputs:\n",
    "           t      batch of targets, one per row\n",
    "           lrate  learning rate\n",
    "        '''\n",
    "        #========== Question 3, part (a) ==========\n",
    "        # TODO: Complete the code below.\n",
    "        # You will have to alter all the lines.\n",
    "        \n",
    "        # Set up top gradient\n",
    "        # since the last layer is a dense layer\n",
    "        network_output = self.lyr[-1]()\n",
    "        self.loss(network_output, t)\n",
    "        dEdh = np.zeros_like(self.lyr[-1]())\n",
    "        dEdh = self.loss.derivative()\n",
    "\n",
    "        # Work our way down through the layers\n",
    "        for i in range(len(self.lyr)-1, 0, -1):\n",
    "\n",
    "            # References to the layer below, and layer above\n",
    "            pre = self.lyr[i-1]   # layer below, (i-1)\n",
    "            post = self.lyr[i]    # layer above, (i)\n",
    "            # Note that:\n",
    "            #   post.L1.W contains the connection weights\n",
    "            #   post.L1.b contains the biases\n",
    "            #   post.L2.z contains the input currents\n",
    "            #   post.L2.h contains the upper layer's activities\n",
    "\n",
    "            # Compute dEdz from dEdh\n",
    "            dEdz = post.L2.act.derivative(dEdh)\n",
    "\n",
    "            # Parameter gradients\n",
    "            dEdW = np.zeros_like(post.L1.W)\n",
    "            if i == 1:\n",
    "                dEdW = np.matmul(pre.h.transpose(), dEdz) \n",
    "            else: \n",
    "                dEdW = np.matmul(pre.L2.h.transpose(), dEdz) \n",
    "\n",
    "            dEdb = np.zeros_like(post.L1.b)\n",
    "            dEdb = np.matmul(np.ones((1, 1000)), dEdz)\n",
    "\n",
    "            # Project gradient through connection, to layer below\n",
    "            dEdh = np.matmul(dEdz, post.L1.W.transpose())\n",
    "\n",
    "            # Update weight parameters\n",
    "            post.L1.W -= (lrate * dEdW)\n",
    "            post.L1.b -= (lrate * dEdb[0])\n",
    "        \n",
    "        \n",
    "    def learn(self, ds, lrate=1., epochs=5000):\n",
    "        '''\n",
    "         net.Learn(ds, lrate=1., epochs=10)\n",
    "\n",
    "         Runs error backpropagation on the network, training on\n",
    "         the data from the Dataset object ds.\n",
    "         \n",
    "         Inputs:\n",
    "           ds       a Dataset object\n",
    "           lrate    learning rate\n",
    "           epochs   number of epochs to run\n",
    "        '''\n",
    "        #========== Question 3, part (b) ==========\n",
    "        # TODO: Complete the code below.\n",
    "        # You will have to edit all these lines.\n",
    "\n",
    "        #=== MIGHT I INTEREST YOU IN ADDING SOME CODE HERE? ===\n",
    "        loss_history = []  # for plotting\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #=== YOU'RE GOING TO WANT TO ADD SOME CODE HERE ===\n",
    "            self(ds.inputs())\n",
    "            self.backprop(ds.targets())\n",
    "            network_output = self.lyr[-1]()\n",
    "            \n",
    "            # Give the poor user some feedback so they know something\n",
    "            # is happening. :)\n",
    "            if epoch%100==0:\n",
    "                cost = self.loss(network_output, ds.targets())\n",
    "                loss_history.append(cost)\n",
    "                print(f'{epoch}: cost = {cost}')\n",
    "\n",
    "        return np.array(loss_history)  # don't touch this line\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code should work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "\n",
    "# Create layers\n",
    "input_layer = Population(2)\n",
    "h1 = DenseLayer(from_nodes=2, to_nodes=30, act=Logistic())\n",
    "h2 = DenseLayer(from_nodes=30, to_nodes=10, act=Logistic())\n",
    "\n",
    "# Only use one of these output_layer/loss combinations\n",
    "\n",
    "# Logistic + CrossEntropy\n",
    "#output_layer = DenseLayer(from_nodes=10, to_nodes=1, act=Logistic())\n",
    "#net.loss = CrossEntropy()\n",
    "\n",
    "# Softmax + Categorical CE\n",
    "output_layer = DenseLayer(from_nodes=10, to_nodes=2, act=Softmax())\n",
    "net.loss = CategoricalCE()\n",
    "\n",
    "# Add layers to the network, from bottom to top\n",
    "net.add_layer(input_layer)\n",
    "net.add_layer(h1)\n",
    "net.add_layer(h2)\n",
    "net.add_layer(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "loss_history = net.learn(ds, epochs=5000);\n",
    "\n",
    "# Plot the progress of the cost\n",
    "plt.plot(loss_history);\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, to see if output match targets\n",
    "y = net(ds.inputs())\n",
    "print(f'Outputs:\\n{y[:5,:]}')\n",
    "print(f'Targets:\\n{ds.targets()[:5,:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cluster plot to make us feel accomplished!\n",
    "ds.plot(labels=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    '''\n",
    "     ac = accuracy(y, t)\n",
    "     \n",
    "     Calculates the fraction of correctly classified samples.\n",
    "     A sample is classified correctly if the largest element\n",
    "     in y corresponds to where the 1 is in the target.\n",
    "     \n",
    "     Inputs:\n",
    "       y  a batch of outputs, with one sample per row\n",
    "       t  the corresponding batch of targets\n",
    "       \n",
    "     Output:\n",
    "       ac the fraction of correct classifications (0<=ac<=1)\n",
    "    '''\n",
    "    true_class = np.argmax(t, axis=1)       # vector of indices for true class\n",
    "    estimated_class = np.argmax(y, axis=1)  # vector of indices for estimated class\n",
    "    errors = sum(true_class==estimated_class)  # add up how many times they match\n",
    "    acc = errors / len(ds)    # divide by the total number of samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = accuracy(net(ds.inputs()), ds.targets())\n",
    "print(f\"Your model's training accuracy = {ac*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
